---
title: "Optimal compression in human concept learning"
collection: publications
permalink: /publication/0_ocm
excerpt: #'This paper is about the number 2. The number 3 is left for future work.' date: 
venue: 'Proceedings of the 65th Annual Meeting of the Cognitive Science Society (CogSci 2023)'
# paperurl: 'https://psyarxiv.com/b62de'
# date: 2023
citation: 'Imel, N. & Zaslavsky, N. (2023). &quot;Optimal compression in human concept learning.&quot; <i>Proceedings of the 46th Annual Meeting of the Cognitive Science Society</i>.'
---

### Abstract

The computational principles that underlie human concept learning have been debated in the literature for decades. Here, we formalize and test a new perspective that is grounded in rate-distortion theory (RDT), the mathematical theory of optimal (lossy) data compression, which has recently been gaining increasing popularity in cognitive science. More specifically, we characterize optimal conceptual systems as solutions to a special type of RDT problem, show how these optimal systems can generalize to unseen examples, and test their predictions for human behavior in three foundational concept-learning experiments. We find converging evidence that optimal compression may account for human concept learning. Our work also lends new insight into the relation between learnability and compressibility; integrates prototype, exemplar, and Bayesian approaches to human concepts within the RDT framework; and offers a potential theoretical link between concept learning and other cognitive functions that have been successfully characterized by efficient compression.
